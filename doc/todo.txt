- Seems paths being reported by 'store' are still not quite right
  (missing parts between given path and last segment?)

- Update documentation
  * Help given by ccouch id <invalid option> is wrong
  * README is out of date
  - doc/ is full of outdated stuff

- RDFify should have option to use alternate RDF writing method for large documents
  - RDFDirectory objects would not be created - data would be serialized more directly
  - Would write standard namespaces out first
  - Non-standard namespace declarations would be included as needed
  - Would write one element at a time to a stream - no need for buffering 

- Metadata stores
  - Any number of metadata stores (similar to how data/ is divided into user-specified 'sectors')
  - Probably backed by lucene
  - Active function to return a list of metadata items from a query
    (has tag X, date < Y, get only the latest 10, sorted by date descending, etc)
  - Active function to create directory from a list of metadata items,
    generating entry targets, names, and modification times based on
    the metadata.
  - Sub-command to import metadata into datastore from RDF files
  - Sub-command to export metadata as RDF based on a query
  - Allow metadata 'documents' to be signed?  Metadata may then be able to take the place of 'heads'.

- RdfNodes ought to read relative URIs as relative to the node's source URI.
  - I don't think I ever did this.
  - This is very low priority, since all RDF docs currently used only contain absolute URIs.

== Done ==

* Create functions to help make photo album pages
  * Cache results of active functions in the datastore, remembering active:... -> urn:... mapping
  * Function to create listing of photos referencing thumbnails
    /process?processor=contentcouch.photoalbum.make-album-page&uri=x-parse-rdf:urn:sha1:OCJIRSUCWLZGHKM5DXHDYQDI5IU6VVTD

* Checkout causes a lot of x-undefined:source URIs to be reported; change to report actual source when possible.

* When comparing files for 'Strict' merge method (which is very useful!),
  use cached content URNs when they are available (e.g. when source URI is
  given or the blob is a FilbBlob and URNs are comparable).
  This used to work but was never re-implemented on RRA branch.

* Remember the repository most recently successfuly downloaded from and
  download from it first.

* Better logging infrastructure
  - Don't show 'GET xxx' message unless logging for that is turned on 

* While caching heads, should store heads in cache datastore, not just heads dir

* When downloading blobs from remote repos, if one repo gives a bad blob, try the next repo instead of just dying.

* A single repository should be able to store separately:
  - data/user   (what the user tells it to store)
  - data/remote (blobs cached from remote repositories)
  - data/active (cached function call results)
  so that users don't have to set up 10 different repositories

* URIs stored in .ccouch-commit-uris should be like x-parse-rdf:urn:sha1:..., not like
  x-parse-rdf:x-ccouch-head:togos-win/togos-image-archives/latest 

* Use TheGetter to get the generic getter.

* RequestHandlers as a more featurified alternative to Getters

* Centralize path handling
  * Be able to create a URI to follow a path into any Directory
  * appendPath('active:xyz', 'ferb/gerb') = 'active:follow-path+source@active:xyz+path@data:,ferb/gerb' 
  * appendPath('foo/bar', 'ferb/gerb') = 'foo/bar/ferb/gerb'
  * option to require '/' after last directory or not for path-based URIs
 
